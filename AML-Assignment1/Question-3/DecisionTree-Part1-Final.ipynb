{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "# Enter You Name Here\n",
    "myname = \"Sharath\" # or \"Doe-Jane-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the initial entropy for a set of rows by converting it into numpy array and passing it to entropy()\n",
    "# to calculate entropy for the set of rows\n",
    "def calculateInitialEntropy(training_set):\n",
    "    training_set=np.asarray(training_set).astype(\"float\")\n",
    "    initial_entropy = entropy(training_set)\n",
    "    return initial_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the information gain based on the parentEntropy and the newly calculated entropy\n",
    "def calculateIG(parentEntropy, entropy):\n",
    "    return (parentEntropy-entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def entropyForm(class0count, class1count):\\n    if(class0count>0 and class1count>0):\\n        probclass0= float(class0count/(class0count+class1count))\\n        probclass1=1-probclass0\\n    else:\\n        probclass0=0\\n        probclass1=0\\n        return 0\\n    entropyCal= probclass0*(-1)*math.log(probclass0)+ probclass1*(-1)*math.log(probclass1)\\n    return entropyCal'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternate function to calculate entropy using the split count of rows based on the two labels\n",
    "'''def entropyForm(class0count, class1count):\n",
    "    if(class0count>0 and class1count>0):\n",
    "        probclass0= float(class0count/(class0count+class1count))\n",
    "        probclass1=1-probclass0\n",
    "    else:\n",
    "        probclass0=0\n",
    "        probclass1=0\n",
    "        return 0\n",
    "    entropyCal= probclass0*(-1)*math.log(probclass0)+ probclass1*(-1)*math.log(probclass1)\n",
    "    return entropyCal'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef getEntropy(training_set, column_index, value):\\n    classg=[row for row in training_set if float(row[column_index])>=value]\\n    classl=[row for row in training_set if float(row[column_index])< value]\\n    count_classg= len(classg)\\n    count_classl=len(classl)\\n    \\n    if(count_classg==0 and count_classl==0):\\n        prob_classg=0\\n        prob_classl=0\\n    else:\\n        prob_classg= float(count_classg/(count_classg+ count_classl))\\n        prob_classl= 1-prob_classg\\n        \\n    classg0= [row for row in classg if float(row[-1])==0]\\n    count_classg0=len(classg0)\\n    classl0= [row for row in classl if float(row[-1])==0]\\n    count_classl0=len(classl0)\\n    count_classg1= len(classg)-count_classg0\\n    count_classl1=len(classl)-count_classl0\\n    \\n    entropyg= entropyForm(count_classg0, count_classg1)\\n    entropyl= entropyForm(count_classl0, count_classl1)\\n    \\n    entropyVal= prob_classg*entropyg+ prob_classl* entropyl\\n    \\n    return entropyVal'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternate function to get the entropy calculation for the column for only 2 labels\n",
    "'''\n",
    "def getEntropy(training_set, column_index, value):\n",
    "    classg=[row for row in training_set if float(row[column_index])>=value]\n",
    "    classl=[row for row in training_set if float(row[column_index])< value]\n",
    "    count_classg= len(classg)\n",
    "    count_classl=len(classl)\n",
    "    \n",
    "    if(count_classg==0 and count_classl==0):\n",
    "        prob_classg=0\n",
    "        prob_classl=0\n",
    "    else:\n",
    "        prob_classg= float(count_classg/(count_classg+ count_classl))\n",
    "        prob_classl= 1-prob_classg\n",
    "        \n",
    "    classg0= [row for row in classg if float(row[-1])==0]\n",
    "    count_classg0=len(classg0)\n",
    "    classl0= [row for row in classl if float(row[-1])==0]\n",
    "    count_classl0=len(classl0)\n",
    "    count_classg1= len(classg)-count_classg0\n",
    "    count_classl1=len(classl)-count_classl0\n",
    "    \n",
    "    entropyg= entropyForm(count_classg0, count_classg1)\n",
    "    entropyl= entropyForm(count_classl0, count_classl1)\n",
    "    \n",
    "    entropyVal= prob_classg*entropyg+ prob_classl* entropyl\n",
    "    \n",
    "    return entropyVal'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entropy for a set of rows using the formula : - summation(p*log2p) where p is the probab of rows that belong to one label\n",
    "def entropy(training_set):\n",
    "    if(training_set.shape[0]==0):\n",
    "        return 0\n",
    "    training_set1=training_set[:,-1]\n",
    "    unique_elements, counts_elements = np.unique(training_set1, return_counts=True)# get the count and type of label in the rows\n",
    "    dic=dict(zip(unique_elements,counts_elements))# create a dictionary with the label and count \n",
    "    p = []\n",
    "    for label in dic:\n",
    "        p.append(dic[label]/float(training_set1.shape[0])) # get the probability of every label into an array so that it is easy for numpy calculation\n",
    "    return -np.sum(np.multiply(p,np.log2(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the entropy for a column by multipling the entropy formula by the probability of split rows\n",
    "def getEntropy(training_set, column_index, value):\n",
    "    training_set_left = [row for row in training_set if float(row[column_index]) >= value]\n",
    "    training_set_right = [row for row in training_set if float(row[column_index])< value]\n",
    "    training_set_left = np.asarray(training_set_left,dtype='float')\n",
    "    training_set_right = np.asarray(training_set_right,dtype='float')\n",
    "    p =float((training_set_left.shape[0])/(training_set_left.shape[0]+training_set_right.shape[0]))\n",
    "    return p*entropy(training_set_left)+(1-p)*entropy(training_set_right)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best split for a set of rows by comparing the entropy of all the columns and choosing the one with\n",
    "#highest information gain\n",
    "def findSplit(training_set):\n",
    "    local_entropy=0\n",
    "    local_column=None\n",
    "    best_entropy=1\n",
    "    best_column=0\n",
    "    col_mean_final=0\n",
    "    bestIg=0\n",
    "    parentEntropy=calculateInitialEntropy(training_set)\n",
    "    for col in range(0,len(training_set[0])-1):\n",
    "        col_data= [float(row[col]) for row in training_set]\n",
    "        col_mean=float(np.sum(col_data)/len(col_data))\n",
    "        true_rows, false_rows= partition(training_set,col,col_mean)\n",
    "        if(len(true_rows) == 0 or len(false_rows)==0):\n",
    "            continue\n",
    "        local_entropy= getEntropy(training_set,col,col_mean)\n",
    "        localIg= calculateIG(parentEntropy,local_entropy)\n",
    "        if localIg>= bestIg:  # store the column only when the information gain of a column is maximum\n",
    "            bestIg=localIg\n",
    "            best_entropy=local_entropy\n",
    "            best_column = col\n",
    "            col_mean_final=col_mean\n",
    "    return  bestIg,best_column,col_mean_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the rows into true and false based on the way in which they satisfy the condition for the column \n",
    "def partition(training_set, column_index, value):\n",
    "    true_rows= [row for row in training_set if float(row[column_index]) >= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])< value]\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class of data in case it cant be split further i.e all the data in this particular class belong to\n",
    "# the same class. So we just take the first label and set it as the output label for the decision tree node\n",
    "def getClass(training_set, column_index,value):\n",
    "    true_label= None\n",
    "    false_label=None\n",
    "    true_rows= [row for row in training_set if float(row[column_index]) >= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])< value]\n",
    "    if len(true_rows)>0:\n",
    "        true_label = true_rows[0][-1]\n",
    "    if len(false_rows)>0:\n",
    "        false_label= false_rows[0][-1]\n",
    "    if false_label==\"1\":\n",
    "        false_label=\"class1\"\n",
    "    if true_label==\"0\":\n",
    "        true_label=\"class0\"\n",
    "    if false_label==\"0\":\n",
    "        false_label=\"class0\"\n",
    "    if true_label==\"1\":\n",
    "        true_label=\"class1\"\n",
    "    if len(true_rows)> len(false_rows):\n",
    "        return true_label\n",
    "    else:\n",
    "        return false_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set,index):\n",
    "        #implement this function\n",
    "        value=0\n",
    "        true_index=0\n",
    "        false_index=0\n",
    "        entropy,column_name,value = findSplit(training_set)\n",
    "        if entropy==0: # Base case when entropy is 0 aka impurity is zero\n",
    "            return getClass(training_set, column_name, value) # we get the class and return it\n",
    "        true_rows, false_rows = partition(training_set,column_name,value)\n",
    "\n",
    "        true_index= self.learn(true_rows, 2*index)\n",
    "        false_index= self.learn(false_rows,2*index+1)\n",
    "        self.tree[index]=[column_name,value,{\"true_rows\":true_index,\"false_rows\":false_index}]\n",
    "        return index\n",
    "\n",
    "    def classify(self, test_instance):\n",
    "        #print(self.tree)\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        index=1\n",
    "        while 1:\n",
    "            value= self.tree[index]\n",
    "            column_num= value[0]\n",
    "            values=value[1]\n",
    "            if float(test_instance[column_num])>=float(values):\n",
    "                result= value[2][\"true_rows\"]\n",
    "            else:\n",
    "                #print(value[2])\n",
    "                result=value[2][\"false_rows\"]\n",
    "            if result==\"class1\" or result==\"class0\":\n",
    "                break\n",
    "            else:\n",
    "                index=result\n",
    "        if result==\"class1\":\n",
    "            result=\"1\"\n",
    "        else:\n",
    "            result=\"0\"\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"assign1-q1-data.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K !=7]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 7]\n",
    "    print(type(training_set))\n",
    "    columns_set={}\n",
    "    tree = DecisionTree()\n",
    "    tree.learn( training_set,1)\n",
    "    results = []\n",
    "#     print(training_set)\n",
    "#     print(test_set)\n",
    "    \n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] ) # prints all values except the last one\n",
    "        results.append( result == instance[-1])\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print(\"Final accuracy: %.4f\" % accuracy)       \n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "<class 'list'>\n",
      "Final accuracy: 0.8265\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
