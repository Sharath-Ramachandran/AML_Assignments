{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization technique that was tried in this experiment was doing ginni impurity calculation instead of entropy calculation. \n",
    "\n",
    "The accuracy result obtained from both entropy and ginni dont differ by much, but ginni is slightly easier to calculate as it doesnot have the log function. \n",
    "\n",
    "Also, ginni impurity is calculated faster as compared to  entropy because of the absence of log function\n",
    "\n",
    "When the tree was created using the entropy impurity(initial entropy), the accuracy obtained was around 82( can be seen in the second ipynb file) but when tried using ginni impurity, the average was slightly lower(81.912(commented code here)) because of all the cross-validation samples, one gave bad accuracy. All the others were almost the same and even some where greater. One example of that is shown in this code(83.67) compared to 82.65 in entropy (shown in first ipynb).\n",
    "\n",
    "The major advantage of ginni over entropy is the fact that it is computationally fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "# Enter You Name Here\n",
    "myname = \"Sharath\" # or \"Doe-Jane-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the initialGinni before the split occcurs by converting it into numpy array and calling\n",
    "# the ginni function that calcuates the ginni value for a set of rows\n",
    "def calculateInitialGinni(training_set):\n",
    "    training_set=np.asarray(training_set).astype(\"float\")\n",
    "    initial_ginni = ginni(training_set)\n",
    "    return initial_ginni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the InformationGain using the ginni values\n",
    "def calculateIG(parentGinni, currentGinni):\n",
    "    return (parentGinni-currentGinni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   def ginniForm(class0count, class1count):\\n    if(class0count>0 and class1count>0):\\n        probclass0= float(class0count/(class0count+class1count))\\n        probclass1=1-probclass0\\n    else:\\n        probclass0=0\\n        probclass1=0\\n        return 0\\n    ginniCal= 1-((probclass0**2)+(probclass1**2))\\n    return ginniCal\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternate method when to calculate ginni when there are just two classes\n",
    "'''\n",
    "   def ginniForm(class0count, class1count):\n",
    "    if(class0count>0 and class1count>0):\n",
    "        probclass0= float(class0count/(class0count+class1count))\n",
    "        probclass1=1-probclass0\n",
    "    else:\n",
    "        probclass0=0\n",
    "        probclass1=0\n",
    "        return 0\n",
    "    ginniCal= 1-((probclass0**2)+(probclass1**2))\n",
    "    return ginniCal\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef ginniVal(training_set,column_index,value):\\n    classg=[row for row in training_set if float(row[column_index])>=value]\\n    classl=[row for row in training_set if float(row[column_index])< value]\\n    count_classg= len(classg)\\n    count_classl=len(classl)\\n    if(count_classg==0 and count_classl==0):\\n        prob_classg=0\\n        prob_classl=0\\n    else:\\n        prob_classg= float(count_classg/(count_classg+ count_classl))\\n        prob_classl= 1-prob_classg\\n        \\n    classg0= [row for row in classg if float(row[-1])==0]\\n    count_classg0=len(classg0)\\n    classl0= [row for row in classl if float(row[-1])==0]\\n    count_classl0=len(classl0)\\n    count_classg1= len(classg)-count_classg0\\n    count_classl1=len(classl)-count_classl0\\n    \\n    entropyg= ginniForm(count_classg0, count_classg1)\\n    entropyl= ginniForm(count_classl0, count_classl1)\\n    \\n    giniIndex= prob_classg*entropyg+ prob_classl* entropyl\\n    \\n    #giniIndex= prob_classg*(1-((prob_g0*prob_g0)+(prob_g1*prob_g1)))+ prob_classl*(1-((prob_l0*prob_l0)+(prob_l1*prob_l1)))\\n    return giniIndex\\n  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternate function for getting ginni for a particular column when without using numpy\n",
    "'''\n",
    "def ginniVal(training_set,column_index,value):\n",
    "    classg=[row for row in training_set if float(row[column_index])>=value]\n",
    "    classl=[row for row in training_set if float(row[column_index])< value]\n",
    "    count_classg= len(classg)\n",
    "    count_classl=len(classl)\n",
    "    if(count_classg==0 and count_classl==0):\n",
    "        prob_classg=0\n",
    "        prob_classl=0\n",
    "    else:\n",
    "        prob_classg= float(count_classg/(count_classg+ count_classl))\n",
    "        prob_classl= 1-prob_classg\n",
    "        \n",
    "    classg0= [row for row in classg if float(row[-1])==0]\n",
    "    count_classg0=len(classg0)\n",
    "    classl0= [row for row in classl if float(row[-1])==0]\n",
    "    count_classl0=len(classl0)\n",
    "    count_classg1= len(classg)-count_classg0\n",
    "    count_classl1=len(classl)-count_classl0\n",
    "    \n",
    "    entropyg= ginniForm(count_classg0, count_classg1)\n",
    "    entropyl= ginniForm(count_classl0, count_classl1)\n",
    "    \n",
    "    giniIndex= prob_classg*entropyg+ prob_classl* entropyl\n",
    "    \n",
    "    #giniIndex= prob_classg*(1-((prob_g0*prob_g0)+(prob_g1*prob_g1)))+ prob_classl*(1-((prob_l0*prob_l0)+(prob_l1*prob_l1)))\n",
    "    return giniIndex\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the ginni value of a partiucular set of rows\n",
    "# does it by splitting the rows according to the labels and obtain the probability of the labels and using the formula\n",
    "#--- 1-summation(p^2)\n",
    "def ginni(training_set):\n",
    "    if(training_set.shape[0]==0):\n",
    "        return 0\n",
    "    training_set1=training_set[:,-1]\n",
    "    unique_elements, counts_elements = np.unique(training_set1, return_counts=True) # get the count and type of label in the rows\n",
    "    dic=dict(zip(unique_elements,counts_elements)) # create a dictionary with the label and count \n",
    "    p = []\n",
    "    for label in dic:\n",
    "        p.append(dic[label]/float(training_set1.shape[0])) # get the probability of every label into an array so that it is easy for numpy calculation\n",
    "    return 1-np.sum(np.multiply(p,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ginni instead of entropy using the formula: (1-summation(p^2)) and returns the \n",
    "#ginni obtained for a particular column\n",
    "def ginniVal(training_set, column_index, value):\n",
    "    training_set_left = [row for row in training_set if float(row[column_index]) >= value]\n",
    "    training_set_right = [row for row in training_set if float(row[column_index])< value]\n",
    "    training_set_left = np.asarray(training_set_left,dtype='float')\n",
    "    training_set_right = np.asarray(training_set_right,dtype='float')\n",
    "    p =float((training_set_left.shape[0])/(training_set_left.shape[0]+training_set_right.shape[0]))\n",
    "    return p*ginni(training_set_left)+(1-p)*ginni(training_set_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best split that is possible for the given training_set by calculating ginni and information gain\n",
    "# then return the column that has the maximum information gain of the other rows\n",
    "def findSplit(training_set):\n",
    "    local_ginni=0\n",
    "    local_column=None\n",
    "    best_ginni=1\n",
    "    best_column=None\n",
    "    col_mean_final=0\n",
    "    bestIg=0\n",
    "    parentGinni=calculateInitialGinni(training_set)\n",
    "\n",
    "    for col in range(0,len(training_set[0])-1):\n",
    "        \n",
    "        col_data= [float(row[col]) for row in training_set]\n",
    "        col_mean=float(np.sum(col_data)/len(col_data)) # get col_mean for the remaining rows\n",
    "        \n",
    "        local_ginni= ginniVal(training_set,col,col_mean)\n",
    "        localIg= calculateIG(parentGinni,local_ginni)\n",
    "        \n",
    "        if localIg>= bestIg:\n",
    "            bestIg=localIg\n",
    "            best_ginni=local_ginni\n",
    "            best_column = col\n",
    "            col_mean_final=col_mean\n",
    "    return  best_ginni,best_column,col_mean_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partition the rows into the rows that follow the condition and others that dont for a column based on a value\n",
    "def partition(training_set, column_index, value):\n",
    "    true_rows= [row for row in training_set if float(row[column_index]) >= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])< value]\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class of data in case it cant be split further i.e all the data in this particular class belong to\n",
    "# the same class. So we just take the first label and set it as the output label for the decision tree node\n",
    "def getClass(training_set, column_index,value):\n",
    "    true_label= None\n",
    "    false_label=None\n",
    "    true_rows= [row for row in training_set if float(row[column_index]) >= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])< value]\n",
    "    \n",
    "    if len(true_rows)>0:\n",
    "        true_label = true_rows[0][-1]\n",
    "    if len(false_rows)>0:\n",
    "        false_label= false_rows[0][-1]\n",
    "        \n",
    "    if false_label==\"1\":\n",
    "        false_label=\"class1\"\n",
    "    if true_label==\"0\":\n",
    "        true_label=\"class0\"\n",
    "    if false_label==\"0\":\n",
    "        false_label=\"class0\"\n",
    "    if true_label==\"1\":\n",
    "        true_label=\"class1\"\n",
    "    if len(true_rows)> len(false_rows):\n",
    "        return true_label\n",
    "    else:\n",
    "        return false_label\n",
    "#     return true_label, false_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set,index):\n",
    "        #implement this function\n",
    "        value=0\n",
    "        true_index=0\n",
    "        false_index=0\n",
    "        gini,column_name,value = findSplit(training_set)\n",
    "    \n",
    "        #check terminating criteria with gini= 0 which means it is evenly being split\n",
    "        if gini==0:\n",
    "            return getClass(training_set, column_name, value)\n",
    "             \n",
    "        # get the true and false rows based on the criteria\n",
    "        true_rows, false_rows = partition(training_set,column_name,value)\n",
    "        \n",
    "        # Make the recursive call for both true and false rows and get the index\n",
    "        true_index = self.learn(true_rows, 2*index)\n",
    "        \n",
    "        false_index= self.learn(false_rows,2*index+1)\n",
    "        \n",
    "        #use the index the insert the data required into the tree\n",
    "       \n",
    "        self.tree[index]=[column_name,value,{\"true_rows\":true_index,\"false_rows\":false_index}]\n",
    "        return index\n",
    "    \n",
    "\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        index=1 # root index\n",
    "        while 1:\n",
    "            value= self.tree[index]\n",
    "            column_num= value[0]\n",
    "            values=value[1]\n",
    "            if float(test_instance[column_num])>=float(values):\n",
    "                result= value[2][\"true_rows\"] # the third item in the tree is a dictionary that contains both the T and F ids\n",
    "            else:\n",
    "                result=value[2][\"false_rows\"]\n",
    "            if result==\"class1\" or result==\"class0\":\n",
    "                break\n",
    "            else:\n",
    "                index=result\n",
    "        if result==\"class1\":\n",
    "            result=\"1\"\n",
    "        else:\n",
    "            result=\"0\"\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"assign1-q1-data.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "    sumOfaccuracy=0\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    #######Comment it for average case testing###################\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 7]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K ==7]\n",
    "    columns_set={}\n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.learn( training_set,1)\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] ) # prints all values except the last one\n",
    "        results.append( result == instance[-1])\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print(\"accuracy: %.4f\" % accuracy)\n",
    "    ###############till-here######################\n",
    "    \n",
    "    #####Un-comment this for average of ginni impurity over 10 sets###########\n",
    "#     K = 10\n",
    "#     sumOfaccuracy=0\n",
    "#     for j in range(0,10):\n",
    "#         training_set = [x for i, x in enumerate(data) if i % K != j]\n",
    "#         test_set = [x for i, x in enumerate(data) if i % K ==j]\n",
    "#         columns_set={}\n",
    "#         tree = DecisionTree()\n",
    "#         # Construct a tree using training set\n",
    "#         tree.learn( training_set,1)\n",
    "\n",
    "#         # Classify the test set using the tree we just constructed\n",
    "#         results = []\n",
    "#         for instance in test_set:\n",
    "#             result = tree.classify( instance[:-1] ) # prints all values except the last one\n",
    "#             results.append( result == instance[-1])\n",
    "#         # Accuracy\n",
    "#         accuracy = float(results.count(True))/float(len(results))\n",
    "#         sumOfaccuracy = sumOfaccuracy+accuracy\n",
    "#         print(\"accuracy: %.4f\" % accuracy)\n",
    "#     print(\"average-accracy for 10 splits is\", float(sumOfaccuracy/10))\n",
    "    ###########till-here######################################\n",
    "    \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8367\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
