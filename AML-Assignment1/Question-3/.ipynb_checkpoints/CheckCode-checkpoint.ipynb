{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "# Enter You Name Here\n",
    "myname = \"Doe-John-Sharath\" # or \"Doe-Jane-\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyFun(training_set, column_index, value):\n",
    "    #print(float(training_set[0][-1]))\n",
    "    col_data_1= [float(row[column_index]) for row in training_set if float(row[-1])==1]\n",
    "    col_data_2= [float(row[column_index]) for row in training_set if float(row[-1])==0]\n",
    "\n",
    "    count_positive_list1= [row for row in col_data_1 if row>= value]\n",
    "    count_positive_class1= len(count_positive_list1)\n",
    "    count_negative_class1= len(col_data_1)- count_positive_class1\n",
    "    \n",
    "    count_positive_list2= [row for row in col_data_2 if row>= value]\n",
    "    count_positive_class2= len(count_positive_list2)\n",
    "    count_negative_class2= len(col_data_2)- count_positive_class2\n",
    "    \n",
    "    count_greater= count_positive_class1 + count_positive_class2\n",
    "    count_lesser = count_negative_class1 + count_negative_class2\n",
    "    \n",
    "#     print(count_positive_class1)\n",
    "#     print(count_positive_class2)\n",
    "#     print(count_negative_class1)\n",
    "#     print(count_negative_class2)\n",
    "    \n",
    "    prob_greater = float(count_greater/(count_greater+count_lesser))\n",
    "    prob_lesser = 1- prob_greater\n",
    "#     print(prob_greater)\n",
    "#     print(prob_lesser)\n",
    "    temp1=1\n",
    "    temp=1\n",
    "    if(count_greater>0):\n",
    "        temp = float(count_positive_class1/ count_greater)\n",
    "    if(count_lesser>0): \n",
    "        temp1 = float(count_negative_class1/ count_lesser)\n",
    "   # if(temp==1 or temp1==1 or temp==0 or temp1==0):\n",
    "     #   entropy_val=0\n",
    "    #else:\n",
    "    #print(temp,\" \",temp1)\n",
    "    if((count_positive_class1>0 or count_positive_class2>0) and (count_negative_class1>0 or count_negative_class2>0)):\n",
    "        if(temp==1 or temp==0 or temp1==1 or temp1==0):\n",
    "            entropy_val=0\n",
    "        elif(count_greater >0 and count_lesser>0):\n",
    "            entropy_val=(prob_greater*(-(temp*math.log(temp,2) + (1-temp)*math.log((1-temp),2)))+prob_lesser*(-(temp1*math.log(temp1,2) + (1-temp1)*math.log((1-temp1),2))))\n",
    "        elif(count_greater>0):\n",
    "            entropy_val=(prob_greater*(-(temp*math.log(temp,2) + (1-temp)*math.log((1-temp),2))))\n",
    "        elif(count_lesser>0):\n",
    "            entropy_val=(prob_lesser*(-(temp*math.log(temp1,2) + (1-temp)*math.log((1-temp1),2))))\n",
    "    else:\n",
    "        print(\"gg\")\n",
    "    #print(\"Entropy is\",entropy_val)\n",
    "    return entropy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInitialEntropy(training_set):\n",
    "    col_data_1= len([row for row in training_set if float(row[-1])==1])\n",
    "    col_data_2= len([row for row in training_set if float(row[-1])==0])\n",
    "    prob_col_data_1= float(col_data_1/(col_data_2+col_data_1))\n",
    "    entropy= -(prob_col_data_1*math.log(prob_col_data_1,2) + (1-prob_col_data_1)*(math.log((1-prob_col_data_1),2)))\n",
    "    print(entropy)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def informationGain(local_entropy, parent_entropy):\\n    print(\"Local entropy::\", local_entropy)\\n    print(\"Parent entropy:: \",parent_entropy)\\n    ig= float(parent_entropy-local_entropy)\\n    return ig'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def informationGain(local_entropy, parent_entropy):\n",
    "    print(\"Local entropy::\", local_entropy)\n",
    "    print(\"Parent entropy:: \",parent_entropy)\n",
    "    ig= float(parent_entropy-local_entropy)\n",
    "    return ig'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSplit(training_set,columns_set,entropy=2):\n",
    "    local_entropy=0\n",
    "    local_column=None\n",
    "    best_entropy=1\n",
    "    best_column=None\n",
    "    #col_mean_final=0\n",
    "    if entropy==2:\n",
    "        entropy= calculateInitialEntropy(training_set)\n",
    "        #print(entropy)\n",
    "    \n",
    "    for col in range(0,len(training_set[0])-1):\n",
    "        #col_data= [float(row[col]) for row in training_set]\n",
    "        col_mean= columns_set[col] \n",
    "        local_entropy= entropyFun(training_set,col,col_mean)\n",
    "        print(\"Local_entropy for col \",col,\" is \",local_entropy)\n",
    "        #print(local_entropy)\n",
    "        if local_entropy< best_entropy:  \n",
    "            best_entropy=local_entropy\n",
    "            best_column = col\n",
    "        \n",
    "        #print(col_data)\n",
    "        #print(best_entropy)\n",
    "    print(best_column)\n",
    "    #ig= informationGain(best_entropy, entropy)\n",
    "    return  best_entropy,best_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(training_set, column_index, value):\n",
    "    true_rows= [row for row in training_set if float(row[column_index])>= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])<value]\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set, columns_set,index, entropy):\n",
    "        # implement this function\n",
    "        self.tree = {}\n",
    "        value=0\n",
    "        true_index=0\n",
    "        false_index=0\n",
    "        entropy,column_name = findSplit(training_set,columns_set,entropy)\n",
    "        value=columns_set[column_name]\n",
    "        print(\"Entropy of column \",column_name,\" is \",entropy)\n",
    "        #check terminating criteria\n",
    "        if entropy==0:\n",
    "            #return the class that is possible\n",
    "            print(\"Base case\")\n",
    "            return 1\n",
    "        # get the true and false rows based on the criteria\n",
    "        true_rows, false_rows = partition(training_set,column_name,value)\n",
    "        #print(column_name)\n",
    "        #print(value)\n",
    "        if(len(true_rows)>0):\n",
    "            print(\"True rows for col \",column_name)\n",
    "            true_index=self.learn(true_rows,columns_set,index+1,entropy)\n",
    "            #true_index= self.learn(true_rows,columns_set,index+1,entropy)\n",
    "        if(len(false_rows)>0):\n",
    "            print(\"Faslse rows for col \",column_name)\n",
    "            false_index = self.learn(false_rows,columns_set, index+2, entropy)\n",
    "        #tree[index]=[column_name, value,true_index,false_index]\n",
    "        #print(tree)\n",
    "        # findsplit function to be called fr both the true as well the false rows\n",
    "        # at the  end we need to make sure it forms the decision node( told in google lecture;; see what it means)\n",
    "        print(\"ColumnName\",column_name)\n",
    "        print(\"Index\",index)\n",
    "        print(\"True_index\", true_index)\n",
    "        print(\"False_index\", false_index)\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        return index\n",
    "    # implement this function\n",
    "\n",
    "\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "col={0: 6.8559990927647885, 1: 0.2773939668859153, 2: 0.3342685416194093, 3: 6.394772057155832, 4: 0.0457108187797688, 5: 35.386935813109545, 6: 138.6946019505557, 7: 0.9940288444091593, 8: 3.188616466318893, 9: 0.4900022680880045}\n",
    "print(len(col.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"assign1-q1-data.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 9]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 9]\n",
    "    \n",
    "    columns_set={}\n",
    "    tree = DecisionTree()\n",
    "    for col in range(0,len(training_set[0])-1):\n",
    "        col_data= [float(row[col]) for row in training_set]\n",
    "        col_mean= sum(col_data)/len(col_data)\n",
    "        columns_set[col]=col_mean\n",
    "    # Construct a tree using training set\n",
    "    #print(columns_set)\n",
    "    tree.learn( training_set,columns_set,1,2)\n",
    "    #print(tree)\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] ) # prints all values except the last one\n",
    "        results.append( result == instance[-1])\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print(\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "0.753494006409005\n",
      "415\n",
      "1628\n",
      "539\n",
      "1827\n",
      "0.4633703787706963\n",
      "0.5366296212293037\n",
      "Local_entropy for col  0  is  0.7528481752744889\n",
      "389\n",
      "1548\n",
      "565\n",
      "1907\n",
      "0.4393286459514629\n",
      "0.5606713540485371\n",
      "Local_entropy for col  1  is  0.7526844902187737\n",
      "356\n",
      "1465\n",
      "598\n",
      "1990\n",
      "0.41301882513041505\n",
      "0.586981174869585\n",
      "Local_entropy for col  2  is  0.7521787746020949\n",
      "291\n",
      "1652\n",
      "663\n",
      "1803\n",
      "0.4406894987525516\n",
      "0.5593105012474484\n",
      "Local_entropy for col  3  is  0.738214210633099\n",
      "196\n",
      "1602\n",
      "758\n",
      "1853\n",
      "0.4078022227262418\n",
      "0.5921977772737582\n",
      "Local_entropy for col  4  is  0.7173380543765617\n",
      "409\n",
      "1598\n",
      "545\n",
      "1857\n",
      "0.4552052619641642\n",
      "0.5447947380358358\n",
      "Local_entropy for col  5  is  0.7529291384736924\n",
      "300\n",
      "1773\n",
      "654\n",
      "1682\n",
      "0.47017464277613974\n",
      "0.5298253572238603\n",
      "Local_entropy for col  6  is  0.7336562011695394\n",
      "216\n",
      "1831\n",
      "738\n",
      "1624\n",
      "0.4642776139714221\n",
      "0.5357223860285779\n",
      "Local_entropy for col  7  is  0.7057523740257372\n",
      "530\n",
      "1577\n",
      "424\n",
      "1878\n",
      "0.4778861419823089\n",
      "0.5221138580176912\n",
      "Local_entropy for col  8  is  0.7486788044627901\n",
      "422\n",
      "1474\n",
      "532\n",
      "1981\n",
      "0.4300294851440236\n",
      "0.5699705148559764\n",
      "Local_entropy for col  9  is  0.753370891010641\n",
      "726\n",
      "1226\n",
      "228\n",
      "2229\n",
      "0.44273077795418464\n",
      "0.5572692220458153\n",
      "Local_entropy for col  10  is  0.6699356832613359\n",
      "10\n",
      "Entropy of column  10  is  0.6699356832613359\n",
      "True rows for col  10\n",
      "266\n",
      "596\n",
      "460\n",
      "630\n",
      "0.4415983606557377\n",
      "0.5584016393442623\n",
      "Local_entropy for col  0  is  0.9422630508769029\n",
      "367\n",
      "522\n",
      "359\n",
      "704\n",
      "0.45543032786885246\n",
      "0.5445696721311475\n",
      "Local_entropy for col  1  is  0.947825080763913\n",
      "271\n",
      "498\n",
      "455\n",
      "728\n",
      "0.39395491803278687\n",
      "0.6060450819672132\n",
      "Local_entropy for col  2  is  0.9513726167792027\n",
      "151\n",
      "291\n",
      "575\n",
      "935\n",
      "0.22643442622950818\n",
      "0.7735655737704918\n",
      "Local_entropy for col  3  is  0.9513028421718898\n",
      "69\n",
      "271\n",
      "657\n",
      "955\n",
      "0.17418032786885246\n",
      "0.8258196721311475\n",
      "Local_entropy for col  4  is  0.9321077336198604\n",
      "289\n",
      "392\n",
      "437\n",
      "834\n",
      "0.3488729508196721\n",
      "0.6511270491803278\n",
      "Local_entropy for col  5  is  0.9476203030978761\n",
      "171\n",
      "331\n",
      "555\n",
      "895\n",
      "0.257172131147541\n",
      "0.742827868852459\n",
      "Local_entropy for col  6  is  0.9510844295237944\n",
      "59\n",
      "168\n",
      "667\n",
      "1058\n",
      "0.11629098360655737\n",
      "0.8837090163934427\n",
      "Local_entropy for col  7  is  0.9467989792637509\n",
      "427\n",
      "596\n",
      "299\n",
      "630\n",
      "0.524077868852459\n",
      "0.475922131147541\n",
      "Local_entropy for col  8  is  0.9450811685227147\n",
      "310\n",
      "482\n",
      "416\n",
      "744\n",
      "0.4057377049180328\n",
      "0.5942622950819672\n",
      "Local_entropy for col  9  is  0.95134051414521\n",
      "726\n",
      "1226\n",
      "0\n",
      "0\n",
      "1.0\n",
      "0.0\n",
      "gg\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'entropy_val' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b77f61f29c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-04dcc5ec6611>\u001b[0m in \u001b[0;36mrun_decision_tree\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Construct a tree using training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(columns_set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#print(tree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Classify the test set using the tree we just constructed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-01693d7d2614>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set, columns_set, index, entropy)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True rows for col \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtrue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m#true_index= self.learn(true_rows,columns_set,index+1,entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-01693d7d2614>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set, columns_set, index, entropy)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfalse_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entropy of column \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a4acb648f824>\u001b[0m in \u001b[0;36mfindSplit\u001b[0;34m(training_set, columns_set, entropy)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#col_data= [float(row[col]) for row in training_set]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcol_mean\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcolumns_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlocal_entropy\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentropyFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Local_entropy for col \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocal_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#print(local_entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2239b4d49642>\u001b[0m in \u001b[0;36mentropyFun\u001b[0;34m(training_set, column_index, value)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#print(\"Entropy is\",entropy_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mentropy_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'entropy_val' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
