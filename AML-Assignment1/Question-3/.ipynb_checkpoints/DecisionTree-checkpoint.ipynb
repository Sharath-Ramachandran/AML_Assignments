{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "# Enter You Name Here\n",
    "myname = \"Doe-John-Sharath\" # or \"Doe-Jane-\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyFun(training_set, column_index, value):\n",
    "    #print(float(training_set[0][-1]))\n",
    "    col_data_1= [float(row[column_index]) for row in training_set if float(row[-1])==1]\n",
    "    col_data_2= [float(row[column_index]) for row in training_set if float(row[-1])==0]\n",
    "\n",
    "    count_positive_list1= [row for row in col_data_1 if row>= value]\n",
    "    count_positive_class1= len(count_positive_list1)\n",
    "    count_negative_class1= len(col_data_1)- count_positive_class1\n",
    "    \n",
    "    count_positive_list2= [row for row in col_data_2 if row>= value]\n",
    "    count_positive_class2= len(count_positive_list2)\n",
    "    count_negative_class2= len(col_data_2)- count_positive_class2\n",
    "    \n",
    "    count_greater= count_positive_class1 + count_positive_class2\n",
    "    count_lesser = count_negative_class1 + count_negative_class2\n",
    "    \n",
    "    prob_greater = float(count_greater/(count_greater+count_lesser))\n",
    "    prob_lesser = 1- prob_greater\n",
    "    temp1=1\n",
    "    temp=1\n",
    "    if(count_greater>0):\n",
    "        temp = float(count_positive_class1/ count_greater)\n",
    "    if(count_lesser>0): \n",
    "        temp1 = float(count_negative_class1/ count_lesser)\n",
    "    if(temp==1 or temp1==1 or temp==0 or temp1==0):\n",
    "        entropy_val=0\n",
    "    else:\n",
    "        entropy_val=(prob_greater*(-(temp*math.log(temp,2) + (1-temp)*math.log((1-temp),2)))+prob_lesser*(-(temp1*math.log(temp1,2) + (1-temp1)*math.log((1-temp1),2))))\n",
    "    #print(\"Entropy is\",entropy_val)\n",
    "    return entropy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInitialEntropy(training_set):\n",
    "    col_data_1= len([row for row in training_set if float(row[-1])==1])\n",
    "    col_data_2= len([row for row in training_set if float(row[-1])==0])\n",
    "    prob_col_data_1= float(col_data_1/(col_data_2+col_data_1))\n",
    "    entropy= -(prob_col_data_1*math.log(prob_col_data_1,2) + (1-prob_col_data_1)*(math.log((1-prob_col_data_1),2)))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(local_entropy, parent_entropy):\n",
    "    print(\"Local entropy::\", local_entropy)\n",
    "    print(\"Parent entropy:: \",parent_entropy)\n",
    "    ig= float(parent_entropy-local_entropy)\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSplit(training_set,columns_set,entropy=2):\n",
    "    local_entropy=0\n",
    "    local_column=None\n",
    "    best_entropy=1\n",
    "    best_column=None\n",
    "    #col_mean_final=0\n",
    "    if entropy==2:\n",
    "        entropy= calculateInitialEntropy(training_set)\n",
    "        #print(entropy)\n",
    "    \n",
    "    for col in range(0,len(training_set[0])-1):\n",
    "        #col_data= [float(row[col]) for row in training_set]\n",
    "        col_mean= columns_set[col] \n",
    "        local_entropy= entropyFun(training_set,col,col_mean)\n",
    "        print(\"Local_entropy for col \",col,\" is \",local_entropy)\n",
    "        #print(local_entropy)\n",
    "        if local_entropy< best_entropy:  \n",
    "            best_entropy=local_entropy\n",
    "            best_column = col\n",
    "        \n",
    "        #print(col_data)\n",
    "        #print(best_entropy)\n",
    "    print(best_column)\n",
    "    ig= informationGain(best_entropy, entropy)\n",
    "    return  local_entropy,ig,best_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(training_set, column_index, value):\n",
    "    true_rows= [row for row in training_set if float(row[column_index])>= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])<value]\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set, columns_set,index, entropy):\n",
    "        # implement this function\n",
    "        self.tree = {}\n",
    "        value=0\n",
    "        true_index=0\n",
    "        false_index=0\n",
    "        entropy,ig,column_name = findSplit(training_set,columns_set,entropy)\n",
    "        value=columns_set[column_name]\n",
    "        #check terminating criteria\n",
    "        if ig==0:\n",
    "            #return the class that is possible\n",
    "            #print(\"Base case\")\n",
    "            return 1\n",
    "        # get the true and false rows based on the criteria\n",
    "        true_rows, false_rows = partition(training_set,column_name,value)\n",
    "        #print(column_name)\n",
    "        #print(value)\n",
    "        if(len(true_rows)>0):\n",
    "            print(\"True rows for column \",column_name)\n",
    "            t1=[]\n",
    "            for row in true_rows:\n",
    "                t1.append(row[:column_name]+row[column_name+1:])\n",
    "            for col in range(column_name-1,len(columns_set.keys())-2):\n",
    "                columns_set[col]=columns_set[col+1]\n",
    "            del columns_set[len(columns_set.keys())-1]\n",
    "            print(columns_set)\n",
    "            \n",
    "            true_index=self.learn(t1,columns_set,index+1,entropy)\n",
    "            #true_index= self.learn(true_rows,columns_set,index+1,entropy)\n",
    "        #if(len(false_rows)>0):\n",
    "         #   print(\"False rows\")\n",
    "          #  false_index = self.learn(false_rows,columns_set, index+2, entropy)\n",
    "        #tree[index]=[column_name, value,true_index,false_index]\n",
    "        #print(tree)\n",
    "        # findsplit function to be called fr both the true as well the false rows\n",
    "        # at the  end we need to make sure it forms the decision node( told in google lecture;; see what it means)\n",
    "        return index\n",
    "    # implement this function\n",
    "\n",
    "\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"assign1-q1-data.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 9]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 9]\n",
    "    \n",
    "    columns_set={}\n",
    "    tree = DecisionTree()\n",
    "    for col in range(0,len(training_set[0])-1):\n",
    "        col_data= [float(row[col]) for row in training_set]\n",
    "        col_mean= sum(col_data)/len(col_data)\n",
    "        columns_set[col]=col_mean\n",
    "    # Construct a tree using training set\n",
    "    print(columns_set)\n",
    "    tree.learn( training_set,columns_set,1,2)\n",
    "    #print(tree)\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] ) # prints all values except the last one\n",
    "        results.append( result == instance[-1])\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print(\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "{0: 6.8559990927647885, 1: 0.2773939668859153, 2: 0.3342685416194093, 3: 6.394772057155832, 4: 0.0457108187797688, 5: 35.386935813109545, 6: 138.6946019505557, 7: 0.9940288444091593, 8: 3.188616466318893, 9: 0.4900022680880045, 10: 10.516777046949441}\n",
      "Local_entropy for col  0  is  0.7528481752744889\n",
      "Local_entropy for col  1  is  0.7526844902187737\n",
      "Local_entropy for col  2  is  0.7521787746020949\n",
      "Local_entropy for col  3  is  0.738214210633099\n",
      "Local_entropy for col  4  is  0.7173380543765617\n",
      "Local_entropy for col  5  is  0.7529291384736924\n",
      "Local_entropy for col  6  is  0.7336562011695394\n",
      "Local_entropy for col  7  is  0.7057523740257372\n",
      "Local_entropy for col  8  is  0.7486788044627901\n",
      "Local_entropy for col  9  is  0.753370891010641\n",
      "Local_entropy for col  10  is  0.6699356832613359\n",
      "10\n",
      "Local entropy:: 0.6699356832613359\n",
      "Parent entropy::  0.753494006409005\n",
      "True rows for column  10\n",
      "{0: 6.8559990927647885, 1: 0.2773939668859153, 2: 0.3342685416194093, 3: 6.394772057155832, 4: 0.0457108187797688, 5: 35.386935813109545, 6: 138.6946019505557, 7: 0.9940288444091593, 8: 3.188616466318893, 9: 0.4900022680880045}\n",
      "Local_entropy for col  0  is  0.9422630508769029\n",
      "Local_entropy for col  1  is  0.947825080763913\n",
      "Local_entropy for col  2  is  0.9513726167792027\n",
      "Local_entropy for col  3  is  0.9513028421718898\n",
      "Local_entropy for col  4  is  0.9321077336198604\n",
      "Local_entropy for col  5  is  0.9476203030978761\n",
      "Local_entropy for col  6  is  0.9510844295237944\n",
      "Local_entropy for col  7  is  0.9467989792637509\n",
      "Local_entropy for col  8  is  0.9450811685227147\n",
      "Local_entropy for col  9  is  0.95134051414521\n",
      "4\n",
      "Local entropy:: 0.9321077336198604\n",
      "Parent entropy::  0.6699356832613359\n",
      "True rows for column  4\n",
      "{0: 6.8559990927647885, 1: 0.2773939668859153, 2: 0.3342685416194093, 3: 0.0457108187797688, 4: 35.386935813109545, 5: 138.6946019505557, 6: 0.9940288444091593, 7: 3.188616466318893, 8: 3.188616466318893}\n",
      "Local_entropy for col  0  is  0.7084617689838345\n",
      "Local_entropy for col  1  is  0.7277156756585867\n",
      "Local_entropy for col  2  is  0.7238805162606948\n",
      "Local_entropy for col  3  is  0\n",
      "Local_entropy for col  4  is  0.7254339281431069\n",
      "Local_entropy for col  5  is  0.7277715772783478\n",
      "Local_entropy for col  6  is  0.7277369211715844\n",
      "Local_entropy for col  7  is  0.7272416970578248\n",
      "Local_entropy for col  8  is  0\n",
      "3\n",
      "Local entropy:: 0\n",
      "Parent entropy::  0.95134051414521\n",
      "True rows for column  3\n",
      "{0: 6.8559990927647885, 1: 0.2773939668859153, 2: 0.0457108187797688, 3: 35.386935813109545, 4: 138.6946019505557, 5: 0.9940288444091593, 6: 3.188616466318893, 7: 3.188616466318893}\n",
      "Local_entropy for col  0  is  0.7084617689838345\n",
      "Local_entropy for col  1  is  0.7277156756585867\n",
      "Local_entropy for col  2  is  0\n",
      "Local_entropy for col  3  is  0.7254339281431069\n",
      "Local_entropy for col  4  is  0.7277715772783478\n",
      "Local_entropy for col  5  is  0.7277369211715844\n",
      "Local_entropy for col  6  is  0.7272416970578248\n",
      "Local_entropy for col  7  is  0\n",
      "2\n",
      "Local entropy:: 0\n",
      "Parent entropy::  0\n",
      "False rows\n",
      "Local_entropy for col  0  is  0.9677155252047944\n",
      "Local_entropy for col  1  is  0.9704611471752422\n",
      "Local_entropy for col  2  is  0.9749554217983978\n",
      "Local_entropy for col  3  is  0\n",
      "Local_entropy for col  4  is  0\n",
      "Local_entropy for col  5  is  0\n",
      "Local_entropy for col  6  is  0\n",
      "Local_entropy for col  7  is  0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b77f61f29c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f229c40b7812>\u001b[0m in \u001b[0;36mrun_decision_tree\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Construct a tree using training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#print(tree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Classify the test set using the tree we just constructed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c2a45d29209b>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set, columns_set, index, entropy)\u001b[0m\n\u001b[1;32m     36\u001b[0m             print(columns_set)'''\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtrue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;31m#true_index= self.learn(true_rows,columns_set,index+1,entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c2a45d29209b>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set, columns_set, index, entropy)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"False rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mfalse_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m#tree[index]=[column_name, value,true_index,false_index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print(tree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c2a45d29209b>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set, columns_set, index, entropy)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfalse_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#check terminating criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-83ccc37d3eed>\u001b[0m in \u001b[0;36mfindSplit\u001b[0;34m(training_set, columns_set, entropy)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#col_data= [float(row[col]) for row in training_set]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcol_mean\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcolumns_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlocal_entropy\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentropyFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Local_entropy for col \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocal_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 8"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
