{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInitialGinni(training_set):\n",
    "    training_set=np.asarray(training_set).astype(\"float\")\n",
    "    initial_ginni = ginni(training_set)\n",
    "    return initial_ginni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIG(parentGinni, currentGinni):\n",
    "    return (parentGinni-currentGinni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ginni(training_set):\n",
    "    if(training_set.shape[0]==0):\n",
    "        return 0\n",
    "    training_set1=training_set[:,-1]\n",
    "    unique_elements, counts_elements = np.unique(training_set1, return_counts=True) # get the count and type of label in the rows\n",
    "    dic=dict(zip(unique_elements,counts_elements)) # create a dictionary with the label and count \n",
    "    p = []\n",
    "    for label in dic:\n",
    "        p.append(dic[label]/float(training_set1.shape[0])) # get the probability of every label into an array so that it is easy for numpy calculation\n",
    "    return 1-np.sum(np.multiply(p,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ginniVal(training_set, column_index, value):\n",
    "    training_set_left = [row for row in training_set if float(row[column_index]) >= value]\n",
    "    training_set_right = [row for row in training_set if float(row[column_index])< value]\n",
    "    training_set_left = np.asarray(training_set_left,dtype='float')\n",
    "    training_set_right = np.asarray(training_set_right,dtype='float')\n",
    "    p =float((training_set_left.shape[0])/(training_set_left.shape[0]+training_set_right.shape[0]))\n",
    "    return p*ginni(training_set_left)+(1-p)*ginni(training_set_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSplit(training_set,featureList):\n",
    "    local_ginni=0\n",
    "    local_column=None\n",
    "    best_ginni=1\n",
    "    best_column=0\n",
    "    col_mean_final=0\n",
    "    bestIg=0\n",
    "    parentGinni=calculateInitialGinni(training_set)\n",
    "    \n",
    "    for col in featureList:\n",
    "        col_data= [float(row[col]) for row in training_set]\n",
    "#         print(\"Column data for \",col, \" is :\",col_data)\n",
    "        col_mean=float(np.sum(col_data)/len(col_data)) # get col_mean for the remaining rows\n",
    "        true_rows, false_rows= partition(training_set,col,col_mean)\n",
    "        if(len(true_rows) == 0 or len(false_rows)==0):\n",
    "            continue\n",
    "        local_ginni= ginniVal(training_set,col,col_mean)\n",
    "        localIg= calculateIG(parentGinni,local_ginni)\n",
    "        if localIg>= bestIg:\n",
    "            bestIg=localIg\n",
    "            best_ginni=local_ginni\n",
    "            best_column = col\n",
    "            col_mean_final=col_mean\n",
    "    return  bestIg,best_column,col_mean_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(training_set, column_index, value):\n",
    "    true_rows= [row for row in training_set if float(row[column_index]) >= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])< value]\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class of data in case it cant be split further i.e all the data in this particular class belong to\n",
    "# the same class. So we just take the first label and set it as the output label for the decision tree node\n",
    "def getClass(training_set, column_index,value):\n",
    "    true_label= None\n",
    "    false_label=None\n",
    "    true_rows= [row for row in training_set if float(row[column_index]) >= value]\n",
    "    false_rows = [row for row in training_set if float(row[column_index])< value]\n",
    "    \n",
    "    if len(true_rows)>0:\n",
    "#         true_rows=np.asarray(true_rows)\n",
    "        true_label = true_rows[0][-1]\n",
    "    if len(false_rows)>0:\n",
    "        false_label= false_rows[0][-1]\n",
    "        \n",
    "    if false_label==1:\n",
    "        false_label=\"class1\"\n",
    "    if true_label==0:\n",
    "        true_label=\"class0\"\n",
    "    if false_label==0:\n",
    "        false_label=\"class0\"\n",
    "    if true_label==1:\n",
    "        true_label=\"class1\"\n",
    "    if len(true_rows)> len(false_rows):\n",
    "        return true_label\n",
    "    else:\n",
    "        return false_label\n",
    "#     return true_label, false_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self):\n",
    "        self.tree = {}\n",
    "    def learn(self, training_set,index,featureList):\n",
    "        #implement this function\n",
    "        value=0\n",
    "        true_index=0\n",
    "        false_index=0\n",
    "        gain,column_name,value = findSplit(training_set,featureList)\n",
    "#         print(\"Value is::\",value,\"for column ::\",column_name)\n",
    "        #check terminating criteria with gini= 0 which means it is evenly being split\n",
    "        if gain==0:\n",
    "            return getClass(training_set, column_name, value)\n",
    "        true_rows, false_rows = partition(training_set,column_name,value)\n",
    "        \n",
    "        # Make the recursive call for both true and false rows and get the index\n",
    "        true_index = self.learn(true_rows, 2*index,featureList)\n",
    "        \n",
    "        false_index= self.learn(false_rows,2*index+1,featureList)\n",
    "        \n",
    "        #use the index the insert the data required into the tree\n",
    "       \n",
    "        self.tree[index]=[column_name,value,{\"true_rows\":true_index,\"false_rows\":false_index}]\n",
    "        return index\n",
    "    \n",
    "\n",
    "    def classify(self, test_instance):\n",
    "#         print(self.tree)\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        index=1 # root index\n",
    "        while 1:\n",
    "            value= self.tree[index]\n",
    "            column_num= value[0]\n",
    "            values=value[1]\n",
    "            if float(test_instance[column_num])>=float(values):\n",
    "                result= value[2][\"true_rows\"] # the third item in the tree is a dictionary that contains both the T and F ids\n",
    "            else:\n",
    "                result=value[2][\"false_rows\"]\n",
    "            if result==\"class1\" or result==\"class0\":\n",
    "                break\n",
    "            else:\n",
    "                index=result\n",
    "        if result==\"class1\":\n",
    "            result=\"1\"\n",
    "        else:\n",
    "            result=\"0\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceDataset(train, features,no_Elements):\n",
    "    # reduce the count of dataset to no_Elements\n",
    "    train= train.sample(n=no_Elements)\n",
    "    random_dataset= train.values\n",
    "    random_subset = train.iloc[:, :-1]\n",
    "    featureList=[i for i in range(random_subset.shape[1])]\n",
    "    selected_features = random.sample(featureList,features)\n",
    "    train=train.values\n",
    "    return train,selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildForest(train, features, count_trees, no_Elements):\n",
    "    forest=[]\n",
    "    for i in range(count_trees):\n",
    "        dataset,featureList= reduceDataset(train, features,no_Elements)\n",
    "#         featureList.sort()\n",
    "        train_dataset=dataset.tolist()\n",
    "        tree = DecisionTree()\n",
    "        tree.learn( train_dataset,1,featureList)\n",
    "        forest.append(tree)\n",
    "        print(\"Done with tree \",(i+1))#,\"Features :\",featureList)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(test, forest):\n",
    "    result_rows=[]\n",
    "    test=test.values.tolist()\n",
    "    for instance in test:\n",
    "        result_inst=[]\n",
    "        for tree in forest:\n",
    "            result=tree.classify(instance)\n",
    "            result_inst.append(result)\n",
    "        result_rows.append(result_inst)\n",
    "    final_result=[]\n",
    "    for i in range(len(result_rows)):\n",
    "        final_result.append(max(set(result_rows[i]), key=result_rows[i].count))\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(len(final_result)):\n",
    "        if float(final_result[i])==(test[i][-1]):\n",
    "            count=count+1\n",
    "    print(\"Accuracy is \",(count*100)/(len(final_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest():\n",
    "    df= pd.read_csv('spam.data.txt',header=None,delimiter=r\"\\s+\")\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    no_Features= train.shape[1]\n",
    "    trimmed_features_count= 55#int(math.sqrt(no_Features)*2)\n",
    "    print(trimmed_features_count)\n",
    "    count_trees=20\n",
    "    no_elements=1000 #2800\n",
    "    forest=buildForest(train,trimmed_features_count,count_trees,no_elements)\n",
    "    getAccuracy(test,forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "Done with tree  1\n",
      "Done with tree  2\n",
      "Done with tree  3\n",
      "Done with tree  4\n",
      "Done with tree  5\n",
      "Done with tree  6\n",
      "Done with tree  7\n",
      "Done with tree  8\n",
      "Done with tree  9\n",
      "Done with tree  10\n",
      "Done with tree  11\n",
      "Done with tree  12\n",
      "Done with tree  13\n",
      "Done with tree  14\n",
      "Done with tree  15\n",
      "Done with tree  16\n",
      "Done with tree  17\n",
      "Done with tree  18\n",
      "Done with tree  19\n",
      "Done with tree  20\n",
      "Accuracy is  93.70249728555918\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random_forest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
